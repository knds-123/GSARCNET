# -*- coding: utf-8 -*-
"""adnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TcEoxT6t1Hd2BfnEBd1IJSzEqpa142AD
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio

class ADNet(nn.Module):
    def __init__(self, n_mfcc=13, d_model=256, hidden_channels=32, max_length=32, sample_rate=48000):
        super(ADNet, self).__init__()
        self.n_mfcc = n_mfcc
        self.d_model = d_model
        self.hidden_channels = hidden_channels
        self.max_length = max_length
        self.sample_rate = sample_rate
        self.conv1 = nn.Conv1d(n_mfcc, hidden_channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(hidden_channels, hidden_channels, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.fc = nn.Linear(hidden_channels * (max_length // 2), d_model)
        self.layernorm = nn.LayerNorm(d_model)
    def forward(self, audio_waveform):
         batch_mfcc = []
        for waveform in audio_waveform:
            mfcc = torchaudio.transforms.MFCC(
                sample_rate=self.sample_rate,
                n_mfcc=self.n_mfcc
            )(waveform)
            if mfcc.size(1) > self.max_length:
                mfcc = mfcc[:, :self.max_length]
            else:
                pad = self.max_length - mfcc.size(1)
                mfcc = F.pad(mfcc, (0, pad))
            batch_mfcc.append(mfcc)
        x = torch.stack(batch_mfcc)
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        B, C, L = x.size()
        x = x.permute(0, 2, 1)  # [B, L, C]
        x = x.reshape(B, L, -1)
        x = self.fc(x)
        x = self.layernorm(x)
        return x
